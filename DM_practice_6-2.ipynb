{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1i8adKFQS7E-"
   },
   "source": [
    "# **Лабораторна робота 6: Пошук аномалій та вирішення задачі *anomaly detection* за допомогою бібліотек `scikit-learn`та `PyTorch`**\n",
    "**Всі завдання виконуються індивідуально. Використання запозиченого коду буде оцінюватись в 0 балів.**\n",
    "\n",
    "**Лабораторні роботи де в коді буде використаня КИРИЛИЦІ будуть оцінюватись в 20 балів.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvneQUbQRRqZ"
   },
   "source": [
    "### Мета роботи:\n",
    "Ознайомитися з основними методами виявлення аномалій, навчитися використовувати бібліотеки `scikit-learn` та `PyTorch` для реалізації алгоритмів пошуку аномалій, проаналізувати ефективність різних методів на реальних наборах даних з Kaggle.\n",
    "\n",
    "\n",
    "### Опис завдання:\n",
    "\n",
    "1. **Постановка задачі**:\n",
    "   Використовуючи один із доступних наборів даних Kaggle (наприклад, *Credit Card Fraud Detection*, *Network Intrusion*, або інші), вам потрібно розв'язати задачу виявлення аномалій. Основна мета — ідентифікувати аномальні записи серед нормальних. Вибраний набір даних повинен містити мітки аномалій для перевірки результатів.\n",
    "\n",
    "2. **Етапи виконання завдання**:\n",
    "   - Завантажте та підготуйте набір даних.\n",
    "   - Проведіть попередню обробку даних (масштабування, заповнення пропущених значень, видалення нерелевантних ознак).\n",
    "   - Використайте різні методи виявлення аномалій:\n",
    "     - **Методи з бібліотеки scikit-learn**:\n",
    "       - Isolation Forest\n",
    "       - One-Class SVM\n",
    "       - Local Outlier Factor (LOF)\n",
    "     - **Методи з використанням PyTorch**:\n",
    "       - Автоенкодери для виявлення аномалій.\n",
    "   - Порівняйте отримані результати, обчисліть метрики якості (Precision, Recall, F1-Score).\n",
    "   - Оцініть, який метод найкраще підходить для вирішення задачі на вашому наборі даних.\n",
    "\n",
    "### Покрокова інструкція\n",
    "\n",
    "1. **Підготовка середовища**:\n",
    "   - Встановіть необхідні бібліотеки:\n",
    "     ```\n",
    "     pip install scikit-learn torch pandas numpy matplotlib\n",
    "     ```\n",
    "\n",
    "2. **Вибір набору даних з Kaggle**:\n",
    "   Зареєструйтесь на Kaggle та оберіть один із наборів даних для виявлення аномалій. Наприклад:\n",
    "   - [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)\n",
    "   - [Network Intrusion Detection](https://www.kaggle.com/xyuanh/benchmarking-datasets)\n",
    "\n",
    "3. **Попередня обробка даних**:\n",
    "   - Завантажте дані та проведіть їхню початкову обробку.\n",
    "   - Масштабуйте ознаки за допомогою `StandardScaler` або `MinMaxScaler`.\n",
    "   - Розділіть дані на навчальну і тестову вибірки.\n",
    "\n",
    "4. **Методи з бібліотеки `scikit-learn`**:\n",
    "\n",
    "   - **Isolation Forest**:\n",
    "     ```\n",
    "     from sklearn.ensemble import IsolationForest\n",
    "     ```\n",
    "\n",
    "   - **One-Class SVM**:\n",
    "     ```\n",
    "     from sklearn.svm import OneClassSVM\n",
    "     ```\n",
    "\n",
    "   - **Local Outlier Factor**:\n",
    "     ```\n",
    "     from sklearn.neighbors import LocalOutlierFactor\n",
    "     ```\n",
    "\n",
    "5. **Методи на основі нейронних мереж (PyTorch)**:\n",
    "\n",
    "   Використайте автоенкодер для пошуку аномалій. Побудуйте нейронну мережу з енкодером і декодером. Під час навчання порівняйте відновлені дані з вхідними та обчисліть помилку. Записи з великою помилкою можуть бути аномаліями.\n",
    "\n",
    "   - **Реалізація автоенкодера**:\n",
    "     ```\n",
    "     import torch\n",
    "     import torch.nn as nn\n",
    "     import torch.optim as optim\n",
    "     ```\n",
    "\n",
    "6. **Оцінка результатів**:\n",
    "   Використовуйте метрики оцінки якості:\n",
    "   - `Precision`, `Recall`, `F1-score`\n",
    "   ```\n",
    "   from sklearn.metrics import classification_report\n",
    "   ```\n",
    "\n",
    "7. **Звіт**:\n",
    "   - Поясніть, який метод дав найкращі результати.\n",
    "   - Проаналізуйте, чому деякі методи працюють краще на вашому наборі даних.\n",
    "   - Оцініть можливості використання глибоких нейронних мереж (автоенкодерів) для вирішення задачі.\n",
    "\n",
    "\n",
    "### Результати, які необхідно надати:\n",
    "1. Код рішення у вигляді Jupyter Notebook з аналізом результатів та поясненнями.\n",
    "\n",
    "\n",
    "### Дедлайн:\n",
    "[27 жовтня 23:59]\n",
    "\n",
    "\n",
    "### Корисні ресурси:\n",
    "- [Документація PyTorch](https://pytorch.org/docs/stable/index.html)\n",
    "- [Документація scikit-learn](https://scikit-learn.org/stable/documentation.html)\n",
    "- [Kaggle Datasets](https://www.kaggle.com/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NeqgMqm2UETO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " Time    0\n",
       " V1      0\n",
       " V2      0\n",
       " V3      0\n",
       " V4      0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "file_path = './creditcard.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data_info = data.info()\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "X = data.drop(columns=['Class'], axis=1)\n",
    "y = data['Class']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "data_info, missing_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Isolation Forest:\n",
      "Confusion Matrix:\n",
      "[[225412   2039]\n",
      " [   154    240]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    227451\n",
      "           1       0.11      0.61      0.18       394\n",
      "\n",
      "    accuracy                           0.99    227845\n",
      "   macro avg       0.55      0.80      0.59    227845\n",
      "weighted avg       1.00      0.99      0.99    227845\n",
      "\n",
      "Accuracy: 0.9903750356602076\n",
      "Results for One-Class SVM:\n",
      "Confusion Matrix:\n",
      "[[221485   5966]\n",
      " [   245    149]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99    227451\n",
      "           1       0.02      0.38      0.05       394\n",
      "\n",
      "    accuracy                           0.97    227845\n",
      "   macro avg       0.51      0.68      0.52    227845\n",
      "weighted avg       1.00      0.97      0.98    227845\n",
      "\n",
      "Accuracy: 0.9727402400754899\n",
      "Results for Local Outlier Factor:\n",
      "Confusion Matrix:\n",
      "[[225183   2268]\n",
      " [   383     11]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    227451\n",
      "           1       0.00      0.03      0.01       394\n",
      "\n",
      "    accuracy                           0.99    227845\n",
      "   macro avg       0.50      0.51      0.50    227845\n",
      "weighted avg       1.00      0.99      0.99    227845\n",
      "\n",
      "Accuracy: 0.9883648971888784\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.01, random_state=42)\n",
    "y_pred_iso = iso_forest.fit_predict(X_train)\n",
    "y_pred_iso = [1 if i == -1 else 0 for i in y_pred_iso]\n",
    "\n",
    "oc_svm = OneClassSVM(nu=0.01, kernel='rbf', gamma=0.1)\n",
    "y_pred_svm = oc_svm.fit_predict(X_train)\n",
    "y_pred_svm = [1 if i == -1 else 0 for i in y_pred_svm]\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)\n",
    "y_pred_lof = lof.fit_predict(X_train)\n",
    "y_pred_lof = [1 if i == -1 else 0 for i in y_pred_lof]\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "\n",
    "evaluate_model(y_train, y_pred_iso, \"Isolation Forest\")\n",
    "evaluate_model(y_train, y_pred_svm, \"One-Class SVM\")\n",
    "evaluate_model(y_train, y_pred_lof, \"Local Outlier Factor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.8193\n",
      "Epoch [2/20], Loss: 0.7359\n",
      "Epoch [3/20], Loss: 0.7203\n",
      "Epoch [4/20], Loss: 0.7134\n",
      "Epoch [5/20], Loss: 0.7082\n",
      "Epoch [6/20], Loss: 0.7060\n",
      "Epoch [7/20], Loss: 0.7038\n",
      "Epoch [8/20], Loss: 0.7034\n",
      "Epoch [9/20], Loss: 0.7022\n",
      "Epoch [10/20], Loss: 0.7012\n",
      "Epoch [11/20], Loss: 0.7005\n",
      "Epoch [12/20], Loss: 0.7002\n",
      "Epoch [13/20], Loss: 0.6996\n",
      "Epoch [14/20], Loss: 0.6998\n",
      "Epoch [15/20], Loss: 0.6993\n",
      "Epoch [16/20], Loss: 0.6991\n",
      "Epoch [17/20], Loss: 0.6992\n",
      "Epoch [18/20], Loss: 0.6992\n",
      "Epoch [19/20], Loss: 0.6990\n",
      "Epoch [20/20], Loss: 0.6990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Autoencoder training complete.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 32\n",
    "model = Autoencoder(input_dim=input_dim, hidden_dim=hidden_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\"Autoencoder training complete.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n      Normal       1.00      1.00      1.00     56864\\n     Anomaly       0.14      0.46      0.22        98\\n\\n    accuracy                           0.99     56962\\n   macro avg       0.57      0.73      0.61     56962\\nweighted avg       1.00      0.99      1.00     56962\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def get_reconstruction_errors(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(data)\n",
    "        mse = torch.mean((data - reconstructed) ** 2, axis=1)\n",
    "    return mse\n",
    "\n",
    "reconstruction_errors = get_reconstruction_errors(model, X_test_tensor)\n",
    "threshold = reconstruction_errors.mean() + 3 * reconstruction_errors.std()\n",
    "\n",
    "y_pred_autoencoder = (reconstruction_errors > threshold).int().numpy()\n",
    "y_test_np = y_test.to_numpy()\n",
    "\n",
    "report = classification_report(y_test_np, y_pred_autoencoder, target_names=[\"Normal\", \"Anomaly\"])\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       1.00      1.00      1.00     56864\n",
      "     Anomaly       0.14      0.46      0.22        98\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.57      0.73      0.61     56962\n",
      "weighted avg       1.00      0.99      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reconstruction_errors = get_reconstruction_errors(model, X_test_tensor)\n",
    "threshold = reconstruction_errors.mean() + 3 * reconstruction_errors.std()\n",
    "\n",
    "y_pred_autoencoder = (reconstruction_errors > threshold).int().numpy()\n",
    "y_test_np = y_test.to_numpy()\n",
    "report = classification_report(y_test_np, y_pred_autoencoder, target_names=[\"Normal\", \"Anomaly\"])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Порівняння результатів моделей.**\n",
    "**Для кожної з моделей було отримано метрики якості: Precision, Recall та F1-score:**\n",
    "\n",
    "Isolation Forest показав найкращі результати для виявлення аномалій серед трьох методів:\n",
    "1. Він досяг найвищого рівня Recall (0.61) для аномалій, що означає, що метод виявляє більше аномалій, ніж інші.\n",
    "\n",
    "2. Попри низьку Precision - 0.11, він забезпечив найбільший баланс між точністю та повнотою F1-score - 0.18 для аномалій.\n",
    "\n",
    "    Отже він є найефективнішим методом для цього набору даних, оскільки він краще справляється з виявленням рідкісних аномалій, хоча і має помірну кількість хибно позитивних спрацьовувань."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Чому деякі методи працюють краще на даних**\n",
    "1. Набір містить чисельні ознаки з сильними кореляціями та різною щільністю в різних кластерах. У таких випадках методи, як LOF, можуть виявитися ефективними, оскільки вони визначають аномалії відносно локальної щільності.\n",
    "2. Аномалії є рідкісними, і дані досить високорозмірні, що робить автоенкодери та Isolation Forest кращими виборами завдяки їх здатності обробляти велику кількість ознак і знаходити відхилення від нормальних шаблонів."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Оцініть можливості використання глибоких нейронних мереж (автоенкодерів) для вирішення задачі**\n",
    "1. Автоенкодери здатні навчитися кодувати складні зв’язки та патерни, що часто є у транзакційних даних. Це дозволяє краще відрізняти нормальні транзакції від аномалій, які значно відхиляються від очікуваних шаблонів.\n",
    "\n",
    "2. Автоенкодери можуть навчитися нелінійним залежностям, що дозволяє виявляти аномалії навіть у складних випадках, де методи LOF або One-Class SVM можуть бути обмеженими.\n",
    "\n",
    "3. Великі дані, як у випадку з фінансовими транзакціями, є відповідними для автоенкодерів. Вони здатні виділити важливі ознаки у компактному латентному просторі, зберігаючи корисну інформацію для виявлення аномалій."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
